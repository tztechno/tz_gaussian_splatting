{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10184619,"sourceType":"datasetVersion","datasetId":6291703},{"sourceId":249324564,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":1366.663154,"end_time":"2025-07-06T10:26:19.216034","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-07-06T10:03:32.55288","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1303502fbf79483ca876c6070eff7ac4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab4e3ff04236400cb4dfab7b257a6aa9","IPY_MODEL_30f37c84a7284ea492f1a45b4d393f19","IPY_MODEL_f0def21052064d61b6622150e8320443"],"layout":"IPY_MODEL_7704bcf9227c45679d1c76337ceab942"}},"ab4e3ff04236400cb4dfab7b257a6aa9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b6ca4289ee1415c9c2b925890016dbf","placeholder":"â€‹","style":"IPY_MODEL_84f1cdeeed8c458b8bbed6fa4cd16dc6","value":"â€‡66%"}},"30f37c84a7284ea492f1a45b4d393f19":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfcaf258b3ef402fa12e2f8a442333e3","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9e1fb2fdc519446f823484bdd6db3d89","value":659}},"f0def21052064d61b6622150e8320443":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dce9c3d7f7ae46f48979be2eef4ffb8e","placeholder":"â€‹","style":"IPY_MODEL_178e4e168e5b4f27804b213f6c644454","value":"â€‡659/1000â€‡[17:22&lt;07:23,â€‡â€‡1.30s/it]"}},"7704bcf9227c45679d1c76337ceab942":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b6ca4289ee1415c9c2b925890016dbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84f1cdeeed8c458b8bbed6fa4cd16dc6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfcaf258b3ef402fa12e2f8a442333e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e1fb2fdc519446f823484bdd6db3d89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dce9c3d7f7ae46f48979be2eef4ffb8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"178e4e168e5b4f27804b213f6c644454":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Drone Video: Gaussian Splatting\npowerd by gemini","metadata":{"papermill":{"duration":0.011104,"end_time":"2025-07-06T10:03:38.132883","exception":false,"start_time":"2025-07-06T10:03:38.121779","status":"completed"},"tags":[],"id":"j4kbAxxRx_Uk"}},{"cell_type":"code","source":"!pip install imageio-ffmpeg\n!pip install pycolmap","metadata":{"execution":{"iopub.status.busy":"2025-07-09T14:27:54.097005Z","iopub.execute_input":"2025-07-09T14:27:54.097289Z","iopub.status.idle":"2025-07-09T14:28:04.061141Z","shell.execute_reply.started":"2025-07-09T14:27:54.09726Z","shell.execute_reply":"2025-07-09T14:28:04.06004Z"},"papermill":{"duration":8.907711,"end_time":"2025-07-06T10:03:47.090263","exception":false,"start_time":"2025-07-06T10:03:38.182552","status":"completed"},"tags":[],"id":"qeSS9Csrx_Ul","outputId":"070afa9c-c549-4bd2-e551-022880570857","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, sys\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport pycolmap\nfrom pathlib import Path\nfrom PIL import Image\nfrom ipywidgets import interactive, widgets","metadata":{"execution":{"iopub.status.busy":"2025-07-09T14:28:04.063671Z","iopub.execute_input":"2025-07-09T14:28:04.064033Z","iopub.status.idle":"2025-07-09T14:28:22.380703Z","shell.execute_reply.started":"2025-07-09T14:28:04.064004Z","shell.execute_reply":"2025-07-09T14:28:22.379753Z"},"id":"bZNXlxmEj0FC","papermill":{"duration":22.721581,"end_time":"2025-07-06T10:04:09.823099","exception":false,"start_time":"2025-07-06T10:03:47.101518","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path = '/kaggle/input/drone-video-pycolmap-camera-position'\nrecon = pycolmap.Reconstruction(f'{path}/sparse/0')\nimage_dir = Path(f'{path}/images/')","metadata":{"trusted":true,"id":"ZntZrl35x_Um","outputId":"3460a13a-f6f8-4839-c990-de47eb43b610","execution":{"iopub.status.busy":"2025-07-09T14:28:22.381616Z","iopub.execute_input":"2025-07-09T14:28:22.382178Z","iopub.status.idle":"2025-07-09T14:28:22.490177Z","shell.execute_reply.started":"2025-07-09T14:28:22.382154Z","shell.execute_reply":"2025-07-09T14:28:22.489176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_images = []\nall_poses = []\nall_intrinsics = []\n\nfor image_id, image in recon.images.items():\n    # Load image\n    img_path = image_dir / image.name\n    img_pil = Image.open(img_path)\n    img = np.array(img_pil) / 255.0\n    all_images.append(img)\n\n    # Intrinsic matrix\n    cam = recon.cameras[image.camera_id]\n    fx, fy = cam.params[0], cam.params[1]\n    cx, cy = cam.params[2], cam.params[3]\n    intrinsics = np.array([\n        [fx, 0, cx],\n        [0, fy, cy],\n        [0,  0,  1]\n    ])\n    all_intrinsics.append(intrinsics)\n\n    # Get camera pose (world-to-camera transformation)\n    cam_fromworld = image.cam_from_world()\n\n    # Extract rotation and translation\n    R = cam_fromworld.rotation.matrix()  # Rotation matrix (3x3)\n    t = cam_fromworld.translation        # Translation vector (3,)\n\n    # Convert to camera-to-world (invert the transformation)\n    c2w = np.eye(4)\n    c2w[:3, :3] = R.T              # Transpose of rotation\n    c2w[:3, 3] = -R.T @ t          # New translation\n    all_poses.append(c2w)\n\nimages = np.stack(all_images)\nposes = np.stack(all_poses)\nintrinsics = np.stack(all_intrinsics)\n\nprint('images.shape:', images.shape)\nprint('poses.shape:', poses.shape)\n\nH, W = images.shape[1:3]\nprint('Image dimensions (H, W):', H, W)\n\n# Extract camera positions\ncamera_positions = poses[:, :3, 3]\nprint(\"First 3 camera positions:\")\nprint(camera_positions[0:3])\n\ncamera_rotations = poses[:, :3, :3]  # Shape: (N, 3, 3)\ncamera_directions = -camera_rotations[:, :, 2]  # Shape: (N, 3)\nradius = np.linalg.norm(camera_positions, axis=1)\nprint(\"Camera distances from origin:\")\nprint(radius)","metadata":{"execution":{"iopub.status.busy":"2025-07-09T14:28:22.491147Z","iopub.execute_input":"2025-07-09T14:28:22.491519Z","iopub.status.idle":"2025-07-09T14:28:26.104455Z","shell.execute_reply.started":"2025-07-09T14:28:22.49149Z","shell.execute_reply":"2025-07-09T14:28:26.103537Z"},"papermill":{"duration":6.300699,"end_time":"2025-07-06T10:04:16.1357","exception":false,"start_time":"2025-07-06T10:04:09.835001","status":"completed"},"tags":[],"id":"1Cw6GalVx_Un","outputId":"56c139eb-6ede-4bfc-fe0f-a1d593617230","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get any image\nsample_image = next(iter(recon.images.values()))\ncamera = recon.cameras[sample_image.camera_id]\n\n# Get camera model information correctly\ncamera_model = camera.model  # This returns the model type directly\nprint('Camera model:', camera_model)\nprint('Camera parameters:', camera.params)\n\n# Extract focal length depending on model\nif camera_model == 'SIMPLE_PINHOLE':\n    focal = camera.params[0]       # [f, cx, cy]\nelif camera_model == 'PINHOLE':\n    fx = camera.params[0]          # [fx, fy, cx, cy]\n    fy = camera.params[1]\n    focal = (fx + fy) / 2\nelif camera_model == 'SIMPLE_RADIAL':\n    focal = camera.params[0]       # [f, cx, cy, k]\nelif camera_model == 'RADIAL':\n    focal = camera.params[0]       # [f, cx, cy, k1, k2]\nelif camera_model == 'OPENCV':\n    fx = camera.params[0]          # [fx, fy, cx, cy, k1, k2, p1, p2]\n    fy = camera.params[1]\n    focal = (fx + fy) / 2\nelif camera_model == 'OPENCV_FISHEYE':\n    fx = camera.params[0]          # [fx, fy, cx, cy, k1, k2, k3, k4]\n    fy = camera.params[1]\n    focal = (fx + fy) / 2\nelse:\n    # For unknown models, assume first parameter is focal length\n    print(f\"Warning: Unknown camera model {camera_model}, assuming first parameter is focal length\")\n    focal = camera.params[0]\n\nprint('Focal length:', focal)","metadata":{"execution":{"iopub.status.busy":"2025-07-09T14:28:26.105446Z","iopub.execute_input":"2025-07-09T14:28:26.105719Z","iopub.status.idle":"2025-07-09T14:28:26.114309Z","shell.execute_reply.started":"2025-07-09T14:28:26.105696Z","shell.execute_reply":"2025-07-09T14:28:26.113165Z"},"papermill":{"duration":0.018764,"end_time":"2025-07-06T10:04:16.165924","exception":false,"start_time":"2025-07-06T10:04:16.14716","status":"completed"},"tags":[],"id":"J2M-56lPx_Un","outputId":"e234446a-f3fd-4489-852e-8fb3d5050500","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{"papermill":{"duration":0.01046,"end_time":"2025-07-06T10:04:16.230168","exception":false,"start_time":"2025-07-06T10:04:16.219708","status":"completed"},"tags":[],"id":"H1vB9WiZx_Uo"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(10,8))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(\n    camera_positions[:, 0],\n    camera_positions[:, 1],\n    camera_positions[:, 2],\n    c='blue',\n    label='Camera Position'\n)\n\nfor i in range(len(poses)):\n    ax.quiver(\n        camera_positions[i, 0],  # point(x)\n        camera_positions[i, 1],  # point(y)\n        camera_positions[i, 2],  # point(z)\n        camera_directions[i, 0],  # vector(x)\n        camera_directions[i, 1],  # vector(y)\n        camera_directions[i, 2],  # vector(z)\n        color='red',\n        length=0.5,\n        arrow_length_ratio=0.1,\n        label='Camera Direction' if i == 0 else None\n    )\n\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.legend()\nplt.title('Camera Positions and Directions in 3D Space')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-09T14:28:26.115268Z","iopub.execute_input":"2025-07-09T14:28:26.115597Z","iopub.status.idle":"2025-07-09T14:28:26.636003Z","shell.execute_reply.started":"2025-07-09T14:28:26.115568Z","shell.execute_reply":"2025-07-09T14:28:26.634984Z"},"papermill":{"duration":0.466713,"end_time":"2025-07-06T10:04:16.707867","exception":false,"start_time":"2025-07-06T10:04:16.241154","status":"completed"},"tags":[],"id":"gITMtopAx_Uo","outputId":"f651f1dd-341c-4667-e3db-4cb6b8c957be","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"testimg, testpose = images[10], poses[10]\nplt.imshow(testimg)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-09T14:28:26.639363Z","iopub.execute_input":"2025-07-09T14:28:26.639735Z","iopub.status.idle":"2025-07-09T14:28:26.939122Z","shell.execute_reply.started":"2025-07-09T14:28:26.639705Z","shell.execute_reply":"2025-07-09T14:28:26.938181Z"},"id":"jj1lof2ej0FI","papermill":{"duration":0.374932,"end_time":"2025-07-06T10:04:17.097566","exception":false,"start_time":"2025-07-06T10:04:16.722634","status":"completed"},"tags":[],"outputId":"03ee0406-1f72-4f7e-9676-c6c36a5beea9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images = images[:100,...,:3] #images[:100, :, :, :3]\nposes = poses[:100]","metadata":{"execution":{"iopub.status.busy":"2025-07-09T14:28:26.94013Z","iopub.execute_input":"2025-07-09T14:28:26.94041Z","iopub.status.idle":"2025-07-09T14:28:26.944858Z","shell.execute_reply.started":"2025-07-09T14:28:26.940386Z","shell.execute_reply":"2025-07-09T14:28:26.943955Z"},"papermill":{"duration":0.02834,"end_time":"2025-07-06T10:04:17.150044","exception":false,"start_time":"2025-07-06T10:04:17.121704","status":"completed"},"tags":[],"id":"gXFvTtpyx_Up","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(testpose.shape)\n\n# Compute camera position (world coordinates)\ncamera_to_world = np.linalg.inv(testpose)\ncam_pos = camera_to_world[:3,3]\nprint(cam_pos)\n\n# Define image plane corners in camera coordinates\nf = 1.0  # focal length (distance along Z)\nimg_width = 1.0\nimg_height = 1.0\n\ncorners_cam = np.array([\n    [-img_width/2, -img_height/2, f, 1],\n    [ img_width/2, -img_height/2, f, 1],\n    [ img_width/2,  img_height/2, f, 1],\n    [-img_width/2,  img_height/2, f, 1],\n]).T  # shape (4,4)\n\n# Transform image plane corners to world coordinates\ncorners_world = camera_to_world @ corners_cam\ncorners_world = corners_world[:3, :].T  # shape (4,3)\n\n# Plotting\nfig = plt.figure(figsize=(8,8))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot camera position\nax.scatter(*cam_pos, color='red', label='Camera Position')\n\n# Plot image plane edges\nfor i in range(4):\n    start = corners_world[i]\n    end = corners_world[(i+1) % 4]\n    ax.plot([start[0], end[0]], [start[1], end[1]], [start[2], end[2]], 'b-')\n\n# Labels and title\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title('Camera position and image plane in 3D')\n\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-09T14:28:26.94615Z","iopub.execute_input":"2025-07-09T14:28:26.946451Z","iopub.status.idle":"2025-07-09T14:28:27.173558Z","shell.execute_reply.started":"2025-07-09T14:28:26.946422Z","shell.execute_reply":"2025-07-09T14:28:27.172578Z"},"papermill":{"duration":0.219701,"end_time":"2025-07-06T10:04:17.575247","exception":false,"start_time":"2025-07-06T10:04:17.355546","status":"completed"},"tags":[],"id":"DM7shz-ax_Up","outputId":"e08d6635-bfc7-4ab5-bad6-8e68ce170c40","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n---","metadata":{"papermill":{"duration":0.026545,"end_time":"2025-07-06T10:04:17.63064","exception":false,"start_time":"2025-07-06T10:04:17.604095","status":"completed"},"tags":[],"id":"RGsn_sZpx_Up"}},{"cell_type":"markdown","source":"## ðŸ”§ Gaussian Splatting","metadata":{"id":"jxDt192E-v6i","papermill":{"duration":0.026078,"end_time":"2025-07-06T10:04:17.681987","exception":false,"start_time":"2025-07-06T10:04:17.655909","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Inverse sigmoid (logit) function\ndef inverse_sigmoid(x):\n    return tf.math.log(x / (1.0 - x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T14:28:27.174716Z","iopub.execute_input":"2025-07-09T14:28:27.175074Z","iopub.status.idle":"2025-07-09T14:28:27.179789Z","shell.execute_reply.started":"2025-07-09T14:28:27.175046Z","shell.execute_reply":"2025-07-09T14:28:27.178914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GaussianSplattingModel(tf.keras.Model):\n    def __init__(self, num_gaussians=10000):\n        super().__init__()\n        self.num_gaussians = num_gaussians\n        \n        # Position initialization (3D coordinates)\n        self.positions = tf.Variable(\n            tf.random.uniform([num_gaussians, 3], -1.0, 1.0),\n            name='positions'\n        )\n        \n        # Rotation initialization (normalized quaternions)\n        quats = tf.random.normal([num_gaussians, 4])\n        self.rotations = tf.Variable(\n            tf.keras.utils.normalize(quats, axis=-1),\n            name='rotations'\n        )\n        \n        # Scale initialization (log scale)\n        self.scales = tf.Variable(\n            tf.math.log(tf.random.uniform([num_gaussians, 3], 0.01, 0.1)),\n            name='scales'\n        )\n        \n        # Opacity initialization (inverse sigmoid)\n        self.opacity = tf.Variable(\n            self.inverse_sigmoid(tf.ones([num_gaussians, 1]) * 0.1),\n            name='opacity'\n        )\n        \n        # Color initialization (inverse sigmoid)\n        self.colors = tf.Variable(\n            self.inverse_sigmoid(tf.random.uniform([num_gaussians, 3])),\n            name='colors'\n        )\n    \n    def inverse_sigmoid(self, x):\n        return tf.math.log(x / (1.0 - x + 1e-6) + 1e-6)\n    \n    def normalize_quaternions(self):\n        self.rotations.assign(tf.keras.utils.normalize(self.rotations, axis=-1))\n    \n    def get_covariance_matrix(self):\n        # Normalize quaternions first\n        q = tf.keras.utils.normalize(self.rotations, axis=-1)\n        w, x, y, z = q[:, 0], q[:, 1], q[:, 2], q[:, 3]\n        \n        # Build rotation matrix from quaternion\n        R = tf.stack([\n            tf.stack([1 - 2*(y**2 + z**2), 2*(x*y - w*z),     2*(x*z + w*y)], axis=1),\n            tf.stack([2*(x*y + w*z),     1 - 2*(x**2 + z**2), 2*(y*z - w*x)], axis=1),\n            tf.stack([2*(x*z - w*y),     2*(y*z + w*x),     1 - 2*(x**2 + y**2)], axis=1),\n        ], axis=1)\n        \n        # Build scale matrix\n        S = tf.linalg.diag(tf.exp(self.scales))\n        \n        # Compute covariance matrix\n        RS = tf.linalg.matmul(R, S)\n        cov = tf.linalg.matmul(RS, tf.transpose(RS, perm=[0, 2, 1]))\n        return cov\n\n    \n#--------\n\n    def render(self, screen_x, screen_y, depths, cov_2d, H, W):\n        # Precompute pixel coordinates (cast to float32 to match center)\n        y_coords, x_coords = tf.meshgrid(\n            tf.range(tf.cast(H, tf.int32)), \n            tf.range(tf.cast(W, tf.int32)),\n            indexing='ij'\n        )\n        pixel_coords = tf.cast(tf.stack([\n            tf.reshape(x_coords, [-1]), \n            tf.reshape(y_coords, [-1])\n        ], axis=1), tf.float32)  # <-- Cast to float32\n        \n        # Sort by depth\n        depth_order = tf.argsort(depths, direction='DESCENDING')\n        num_gaussians = tf.minimum(self.num_gaussians, 10000)\n        \n        # Initialize accumulators\n        image = tf.zeros([H * W, 3], dtype=tf.float32)\n        alpha_acc = tf.zeros([H * W], dtype=tf.float32)\n        \n        # Convert to while_loop compatible format\n        def body(i, image, alpha_acc, continue_rendering):\n            idx = depth_order[i]\n            center = tf.stack([screen_x[idx], screen_y[idx]])  # Already float32\n            \n            # Boundary check\n            in_bounds = tf.reduce_all([\n                center[0] > -50.0,  # <-- Use float literals\n                center[0] < tf.cast(W, tf.float32) + 50.0,\n                center[1] > -50.0,\n                center[1] < tf.cast(H, tf.float32) + 50.0\n            ])\n            \n            def process_gaussian():\n                diff_full = pixel_coords - center  # Now both float32\n                dist2 = tf.reduce_sum(tf.square(diff_full), axis=1)\n                mask = dist2 < (20.0 ** 2)\n                active_coords = tf.boolean_mask(pixel_coords, mask)\n                active_idx = tf.where(mask)[:, 0]\n                \n                cov = cov_2d[idx] + 1e-3 * tf.eye(2)\n                inv_cov = tf.cond(\n                    tf.abs(tf.linalg.det(cov)) > 1e-6,\n                    lambda: tf.linalg.inv(cov),\n                    lambda: tf.linalg.pinv(cov)\n                )\n                \n                diff = active_coords - center\n                maha_sq = tf.reduce_sum(\n                    tf.multiply(tf.linalg.matmul(diff, inv_cov), diff), \n                    axis=1\n                )\n                weight = tf.exp(-0.5 * maha_sq)\n                \n                alpha = tf.sigmoid(self.opacity[idx, 0]) * weight\n                transmittance = tf.gather(1.0 - alpha_acc, active_idx)\n                alpha_blend = alpha * transmittance\n                \n                color = tf.sigmoid(self.colors[idx])\n                delta_image = alpha_blend[:, None] * color[None, :]\n                \n                return (\n                    tf.tensor_scatter_nd_add(image, active_idx[:, None], delta_image),\n                    tf.tensor_scatter_nd_add(alpha_acc, active_idx[:, None], alpha_blend)\n                )\n            \n            image_out, alpha_acc_out = tf.cond(\n                in_bounds,\n                process_gaussian,\n                lambda: (image, alpha_acc)\n            )\n            \n            # Early termination condition\n            continue_rendering = tf.logical_and(\n                continue_rendering,\n                tf.reduce_mean(alpha_acc_out) <= 0.99\n            )\n            \n            return i+1, image_out, alpha_acc_out, continue_rendering\n        \n        # Run the loop\n        _, final_image, final_alpha, _ = tf.while_loop(\n            cond=lambda i, img, acc, cont: tf.logical_and(i < num_gaussians, cont),\n            body=body,\n            loop_vars=(\n                tf.constant(0), \n                image, \n                alpha_acc, \n                tf.constant(True)\n            ),\n            maximum_iterations=num_gaussians\n        )\n        \n        return tf.reshape(final_image, [H, W, 3])\n\n    def call(self, inputs, training=None):\n        \"\"\"\n        Args:\n            inputs: dict containing:\n                - 'view_matrix': 4x4 view matrix (tf.float32)\n                - 'focal': focal length (tf.float32)\n                - 'height': image height (tf.int32)\n                - 'width': image width (tf.int32)\n        \"\"\"\n        # Ensure consistent data types for inputs\n        view_matrix = tf.cast(inputs['view_matrix'], tf.float32)\n        focal = tf.cast(inputs['focal'], tf.float32)\n        H = tf.cast(inputs['height'], tf.float32)  # Convert to float32\n        W = tf.cast(inputs['width'], tf.float32)   # Convert to float32\n    \n        # Perform rendering\n        screen_x, screen_y, depths, cov_2d = self.project_gaussians(view_matrix, focal, H, W)\n        return self.render(screen_x, screen_y, depths, cov_2d, H, W)\n    \n    def project_gaussians(self, view_matrix, focal, H, W):\n        # Explicitly unify data types\n        view_matrix = tf.cast(view_matrix, tf.float32)\n        focal = tf.cast(focal, tf.float32)\n        H = tf.cast(H, tf.float32)\n        W = tf.cast(W, tf.float32)\n    \n        # Convert 3D positions to homogeneous coordinates\n        positions_h = tf.concat([self.positions, tf.ones([self.num_gaussians, 1])], axis=1)\n        cam_coords_h = tf.linalg.matmul(positions_h, tf.transpose(view_matrix))\n        cam_coords = cam_coords_h[:, :3] / (cam_coords_h[:, 3:4] + 1e-8)  # Normalize by homogeneous w\n        depths = cam_coords[:, 2]\n    \n        # Confirm and enforce float32 types\n        cam_coords = tf.cast(cam_coords, tf.float32)\n        depths = tf.cast(depths, tf.float32)\n    \n        # Project to screen coordinates\n        screen_x = (cam_coords[:, 0] / (depths + 1e-8)) * focal + W / 2\n        screen_y = (cam_coords[:, 1] / (depths + 1e-8)) * focal + H / 2\n    \n        # Compute 3D covariance matrices (user-defined method)\n        cov_3d = self.get_covariance_matrix()\n    \n        # Transform covariance into camera space\n        R_cam = view_matrix[:3, :3]\n        cov_cam = tf.linalg.matmul(tf.linalg.matmul(R_cam, cov_3d), tf.transpose(R_cam))\n    \n        # Jacobian for perspective projection\n        z_sq = depths ** 2\n        J = tf.stack([\n            tf.stack([focal / depths, tf.zeros_like(depths), -focal * cam_coords[:, 0] / z_sq], axis=-1),\n            tf.stack([tf.zeros_like(depths), focal / depths, -focal * cam_coords[:, 1] / z_sq], axis=-1)\n        ], axis=1)\n    \n        # Project covariance to 2D image space\n        cov_2d = tf.linalg.matmul(tf.linalg.matmul(J, cov_cam), tf.transpose(J, perm=[0, 2, 1]))\n        \n        return screen_x, screen_y, depths, cov_2d\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T14:28:27.180921Z","iopub.execute_input":"2025-07-09T14:28:27.181353Z","iopub.status.idle":"2025-07-09T14:28:27.207618Z","shell.execute_reply.started":"2025-07-09T14:28:27.181325Z","shell.execute_reply":"2025-07-09T14:28:27.206205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function\ndef train_step(img_idx, pose, pixel_coords, target_rgb, model, focal, H, W, optimizer):\n    target_rgb = tf.cast(target_rgb, tf.float32)\n\n    with tf.GradientTape() as tape:\n        inputs = {\n            'view_matrix': tf.cast(tf.linalg.inv(pose), tf.float32),\n            'focal': tf.cast(focal, tf.float32),\n            'height': tf.cast(H, tf.float32), \n            'width': tf.cast(W, tf.float32)    \n        }\n        \n        rendered_image = model(inputs)\n        rendered_pixels = tf.gather_nd(rendered_image, pixel_coords)\n        loss = tf.reduce_mean(tf.square(rendered_pixels - target_rgb))\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n    gradients = [tf.clip_by_value(g, -1.0, 1.0) if g is not None else g for g in gradients]\n    grads_and_vars = [(g, v) for g, v in zip(gradients, model.trainable_variables) if g is not None]\n\n    if grads_and_vars:\n        optimizer.apply_gradients(grads_and_vars)\n    else:\n        print(\"Warning: No gradients to apply!\")\n\n    model.normalize_quaternions()\n    return loss, rendered_pixels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T14:28:27.208394Z","iopub.status.idle":"2025-07-09T14:28:27.208721Z","shell.execute_reply.started":"2025-07-09T14:28:27.208567Z","shell.execute_reply":"2025-07-09T14:28:27.208581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gaussian Splatting Model old\n# Training data preparation (reuse from NeRF)\ndef get_training_batch(images, poses, focal, H, W, N_rand=1024):\n    \"\"\"Randomly sample pixels for training\"\"\"\n    # Randomly select an image\n    img_i = np.random.choice(len(images))\n    img = images[img_i]\n    pose = poses[img_i]\n\n    # Ensure pose is float32\n    pose = tf.cast(pose, tf.float32)\n\n    # Randomly sample pixels\n    coords = tf.stack(tf.meshgrid(tf.range(H), tf.range(W), indexing='ij'), -1)\n    coords = tf.reshape(coords, [-1, 2])\n    select_inds = tf.random.uniform([N_rand], maxval=coords.shape[0], dtype=tf.int32)\n    select_coords = tf.gather(coords, select_inds)\n\n    target_batch = tf.gather_nd(img, select_coords)\n\n    return img_i, pose, select_coords, target_batch","metadata":{"trusted":true,"id":"FQywPtdhx_Uq","outputId":"09be63bd-ae24-4172-81c1-058f50d3d8f7","execution":{"iopub.status.busy":"2025-07-09T14:28:27.209641Z","iopub.status.idle":"2025-07-09T14:28:27.20998Z","shell.execute_reply.started":"2025-07-09T14:28:27.209801Z","shell.execute_reply":"2025-07-09T14:28:27.209818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training parameters\nN_iters = 1000\nN_rand = 1024\nlearning_rate = 0.008 #1e-3,1e-4\ntestimg_idx = 10\n\n# Initialize model\nmodel = GaussianSplattingModel(num_gaussians=1000)\n\n# Optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate)\n\n# Training loop\nprint(\"Starting Gaussian Splatting training...\")\nprint(f\"Number of Gaussians: {model.num_gaussians}\")\nprint(f\"Number of training iterations: {N_iters}\")","metadata":{"trusted":true,"id":"FQywPtdhx_Uq","outputId":"09be63bd-ae24-4172-81c1-058f50d3d8f7","execution":{"iopub.status.busy":"2025-07-09T14:28:27.211657Z","iopub.status.idle":"2025-07-09T14:28:27.212007Z","shell.execute_reply.started":"2025-07-09T14:28:27.211822Z","shell.execute_reply":"2025-07-09T14:28:27.211835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"psnr_list = []\nloss_list = []\nbest_psnr = 0\nbest_weights = None\n\nfor i in tqdm(range(N_iters), desc=\"Training\"):\n    # Training batch\n    img_i, pose, pixel_coords, target_rgb = get_training_batch(images, poses, focal, H, W, N_rand)\n    \n    # Training step\n    loss, rendered_pixels = train_step(img_i, pose, pixel_coords, target_rgb, model, focal, H, W, optimizer)\n    loss_list.append(loss.numpy())\n\n    # Evaluation\n    if i % 100 == 0:\n        tqdm.write(f\"Step {i:06d}: Loss = {loss:.4f}\")\n        \n        if i % 500 == 0:\n            # Prepare test inputs\n            test_pose = poses[testimg_idx]\n            inputs = {\n                'view_matrix': tf.cast(tf.linalg.inv(test_pose), tf.float32),\n                'focal': tf.cast(focal, tf.float32),\n                'height': tf.cast(H, tf.float32),\n                'width': tf.cast(W, tf.float32)\n            }\n            \n            # Render using the model directly\n            rendered_test = model(inputs)\n            target_test = images[testimg_idx]\n            \n            # Calculate PSNR\n            mse = tf.reduce_mean(tf.square(rendered_test - target_test))\n            psnr = -10. * tf.math.log(mse) / tf.math.log(10.)\n            psnr_list.append(psnr.numpy())\n            \n            # Visualization\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n            ax1.imshow(target_test)\n            ax1.set_title(\"Ground Truth\")\n            ax2.imshow(np.clip(rendered_test, 0, 1))\n            ax2.set_title(f\"Iter {i}, PSNR: {psnr:.2f}\")\n            plt.show()\n            plt.close(fig)\n","metadata":{"trusted":true,"id":"FQywPtdhx_Uq","outputId":"09be63bd-ae24-4172-81c1-058f50d3d8f7","execution":{"iopub.status.busy":"2025-07-09T14:28:27.213557Z","iopub.status.idle":"2025-07-09T14:28:27.213944Z","shell.execute_reply.started":"2025-07-09T14:28:27.213763Z","shell.execute_reply":"2025-07-09T14:28:27.21378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training curves\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(loss_list)\nplt.title(\"Training Loss\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.yscale('log')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(0, len(psnr_list) * 500, 500), psnr_list)\nplt.title(\"Test PSNR\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"PSNR (dB)\")\nplt.tight_layout()\nplt.show()\n\n# Final test rendering\nprint(\"Performing final test rendering...\")\n\nfinal_rendered = model({\n    'view_matrix': tf.constant(poses[testimg_idx], dtype=tf.float32),\n    'focal': tf.constant(focal, dtype=tf.float32),\n    'height': tf.constant(H, dtype=tf.float32),\n    'width': tf.constant(W, dtype=tf.float32)\n})\n\nfinal_rendered = tf.cast(final_rendered, tf.float32).numpy()\ntarget_image = tf.cast(images[testimg_idx], tf.float32).numpy() if isinstance(images[testimg_idx], tf.Tensor) else images[testimg_idx].astype(np.float32)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(images[testimg_idx])\nplt.title(\"Ground Truth\")\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(np.clip(final_rendered, 0, 1))\nplt.title(\"Gaussian Splatting Rendered\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"id":"aNwSAFXRx_Ur","execution":{"iopub.status.busy":"2025-07-09T14:28:27.215123Z","iopub.status.idle":"2025-07-09T14:28:27.215487Z","shell.execute_reply.started":"2025-07-09T14:28:27.215281Z","shell.execute_reply":"2025-07-09T14:28:27.215308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final PSNR calculation\nfinal_mse = tf.reduce_mean(tf.square(final_rendered - target_image))\nfinal_psnr = -10. * tf.math.log(final_mse) / tf.math.log(tf.constant(10., dtype=tf.float32))\nprint(f\"Final PSNR: {final_psnr:.2f} dB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n---","metadata":{"papermill":{"duration":0.121856,"end_time":"2025-07-06T10:23:20.794542","exception":false,"start_time":"2025-07-06T10:23:20.672686","status":"completed"},"tags":[],"id":"mu0b3I1Nx_Ur"}},{"cell_type":"markdown","source":"## ðŸ”§ Interactive Visualization","metadata":{"id":"bZLEFNox_UVK","papermill":{"duration":0.124697,"end_time":"2025-07-06T10:23:21.291211","exception":false,"start_time":"2025-07-06T10:23:21.166514","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(H,W)","metadata":{"papermill":{"duration":0.128454,"end_time":"2025-07-06T10:23:21.54494","exception":false,"start_time":"2025-07-06T10:23:21.416486","status":"completed"},"tags":[],"id":"9ynRBZsSx_Ur","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T14:28:27.217329Z","iopub.status.idle":"2025-07-09T14:28:27.218313Z","shell.execute_reply.started":"2025-07-09T14:28:27.218163Z","shell.execute_reply":"2025-07-09T14:28:27.218179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Camera pose generation functions (used as-is)\ntrans_t = lambda t : tf.convert_to_tensor([\n    [1,0,0,0],\n    [0,1,0,0],\n    [0,0,1,t],\n    [0,0,0,1],\n], dtype=tf.float32)\n\nrot_phi = lambda phi : tf.convert_to_tensor([\n    [1,0,0,0],\n    [0,tf.cos(phi),-tf.sin(phi),0],\n    [0,tf.sin(phi), tf.cos(phi),0],\n    [0,0,0,1],\n], dtype=tf.float32)\n\nrot_theta = lambda th : tf.convert_to_tensor([\n    [tf.cos(th),0,-tf.sin(th),0],\n    [0,1,0,0],\n    [tf.sin(th),0, tf.cos(th),0],\n    [0,0,0,1],\n], dtype=tf.float32)\n\ndef pose_spherical(theta, phi, radius):\n    c2w = trans_t(radius)\n    c2w = rot_phi(phi / 180. * np.pi) @ c2w\n    c2w = rot_theta(theta / 180. * np.pi) @ c2w\n    c2w = np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]]) @ c2w\n    return c2w\n\n\n# Rendering function for Gaussian Splatting\ndef render_view(model, c2w, H, W, focal):\n    inputs = {\n        'view_matrix': tf.cast(tf.linalg.inv(c2w), tf.float32),\n        'focal': tf.cast(focal, tf.float32),\n        'height': tf.cast(H//5, tf.float32),  # float32ã«çµ±ä¸€\n        'width': tf.cast(W//5, tf.float32)    # float32ã«çµ±ä¸€\n    }\n    return model(inputs).numpy()\n\ndef f(**kwargs):\n    c2w = pose_spherical(**kwargs)\n    # Direct rendering instead of get_rays\n    img = render_view(model, c2w, H, W, focal)\n\n    plt.figure(2, figsize=(20, 6))\n    plt.imshow(np.clip(img, 0, 1))\n    plt.show()\n\n# Slider configuration\nsldr = lambda v, mi, ma: widgets.FloatSlider(\n    value=v,\n    min=mi,\n    max=ma,\n    step=.01,\n)\n\nnames = [\n    ['theta', [100., 0., 360]],\n    ['phi', [-30., -90, 0]],\n    ['radius', [4., 3., 5.]],\n]\n\ninteractive_plot = interactive(f, **{s[0]: sldr(*s[1]) for s in names})\noutput = interactive_plot.children[-1]\noutput.layout.height = '350px'\ninteractive_plot\n","metadata":{"trusted":true,"id":"yoUMlHZHx_Ur","execution":{"iopub.status.busy":"2025-07-09T14:28:27.219144Z","iopub.status.idle":"2025-07-09T14:28:27.219516Z","shell.execute_reply.started":"2025-07-09T14:28:27.219385Z","shell.execute_reply":"2025-07-09T14:28:27.219399Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ”§ Render 360 Video","metadata":{"id":"PpKhAn2a__Iu","papermill":{"duration":0.123393,"end_time":"2025-07-06T10:23:26.933214","exception":false,"start_time":"2025-07-06T10:23:26.809821","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nframes = []\nfor th in tqdm(np.linspace(0., 360., 36, endpoint=False)):\n    c2w = pose_spherical(th, -30., 4.)\n    img = render_view(model, c2w, H//5, W//5, focal)\n    frames.append((255*np.clip(img, 0, 1)).astype(np.uint8))\n\nimport imageio\nf = 'video.mp4'\nimageio.mimwrite(f, frames, fps=4, quality=10)\n\nfrom IPython.display import HTML\nfrom base64 import b64encode\nmp4 = open('video.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=400 controls autoplay loop>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"trusted":true,"id":"_1sId4OBx_Us","execution":{"iopub.status.busy":"2025-07-09T14:28:27.220809Z","iopub.status.idle":"2025-07-09T14:28:27.221209Z","shell.execute_reply.started":"2025-07-09T14:28:27.221023Z","shell.execute_reply":"2025-07-09T14:28:27.221041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"CT38DMfyCICr","papermill":{"duration":0.131306,"end_time":"2025-07-06T10:26:15.781188","exception":false,"start_time":"2025-07-06T10:26:15.649882","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.135981,"end_time":"2025-07-06T10:26:16.055194","exception":false,"start_time":"2025-07-06T10:26:15.919213","status":"completed"},"tags":[],"id":"c8yN7hVXx_Us","trusted":true},"outputs":[],"execution_count":null}]}